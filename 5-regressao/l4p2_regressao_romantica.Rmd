---
title: "l4p2_regressao_romantica"
author: "devjaynemorais"
date: "8 de julho de 2019"
output: 
  html_notebook:
    theme: lumen
    fig_width: 7
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: inline
---
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(lubridate)
library(ggbeeswarm)
library(ggridges)
library(ggplot2)
library(scales)

library(GGally)
library(pscl)
library(broom)
library(modelr) # devtools::install_github("hadley/modelr")

theme_set(theme_bw())
```


```{r}
dados = read_csv("speed-dating/speed-dating2.csv")

dados = dados %>% mutate(match = ifelse(dec == "yes", 1, 0))
```

Análise exploratória dos dados:

```{r}
dados  %>%
  ggplot(aes(x = dec, fill = dec)) + 
    geom_bar(width=.5) +
    labs(
        title = "Distribuição de Match",
        y = "Quantidade",
        x = "Match" 
      ) + 
  guides(fill=guide_legend("Deu Match?"))


```
É possível observar que há uma quantidade bem menor de match correspondido do que não correspondido.

```{r}
dados %>%
  #na.omit(samerace) %>%
  ggplot(aes(x = like)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  scale_y_continuous(breaks=seq(0, 10000, by = 100)) +
  labs(x= "Quanto p1 gostou de p2 (like)",
       y = "Quantidade")
``` 

```{r}
dados %>%
  #na.omit(samerace) %>%
  ggplot(aes(x = intel)) +
  geom_histogram(color = "black",
                 fill = "green",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  scale_y_continuous(breaks=seq(0, 10000, by = 100)) +
  labs(x= "Quão inteligente p1 achou p2 (intel)",
       y = "Quantidade")
``` 
```{r}
dados %>%
  #na.omit(samerace) %>%
  ggplot(aes(x = attr)) +
  geom_histogram(color = "black",
                 fill = "pink",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  scale_y_continuous(breaks=seq(0, 10000, by = 100)) +
  labs(x= "Quão atraente p1 achou p2 (attr)",
       y = "Quantidade")
``` 

```{r}
dados  %>%
  ggplot(aes(y=prob, x=int_corr)) +
  geom_point(alpha = 0.1, position = position_jitter(width = 0.25), color="purple") +
  labs(
    title="C", 
    x= "Correlação entre os interesses entre as pessoas", 
    y= "Probabilidade de um novo encontro"
  ) +
  scale_x_log10() 

```  
  
Para esta análise serão escolhidas as seguintes variáveis:

`prob` : que probabiliade p1 acha que p2 tem de querer se encontrar novamente com p- (escala 1-10)
`int_corr` : correlação entre os interesses de p1 e p2
`like` : no geral, quanto p1 gostou de p2?
`intel` : quão inteligente p1 achou p2
`attr` : quão atraente p1 achou p2
--------------------------------------------------
`fun` : quão divertido p1 achou p2
`shar` : quanto p1 achou que compartilha interesses e hobbies com p2
`sinc` : quão sincero p1 achou p2
`samerace` : p1 e p2 são da mesma raça?
`intel` : quão inteligente p1 achou p2
`amb` : quão ambicioso p1 achou p2  

```{r}

#dados = dados %>% augment(type.predict = "response")

bm <- glm(match ~ int_corr + prob + like + intel + attr,
      data = dados,  
      family = "binomial")


tidy(bm, conf.int = TRUE)
tidy(bm, conf.int = TRUE, exponentiate = TRUE)

glance(bm)
pR2(bm)
``` 
Com 95% de confiança.

```{r}
tidy(bm, conf.int = TRUE, conf.level = 0.95) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high)) +
  geom_bar(stat = "identity", position = position_dodge(0.8), fill="#00b894", alpha=0.5, width=.7) + 
  geom_errorbar(size = 0.8, width= 0.2) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 0, colour = "darkred") +
  labs(x = "Variáveis Independentes",
       title = "Intervalos - regressão logística",
       y = expression("Coeficientes"))
``` 

# Que fatores nos dados têm efeito relevante na chance do casal ter um match? Descreva se os efeitos são positivos ou negativos e sua magnitude. 


### Conclusão

Na amostra de dados analisada foi utilizada regressão múltipla logistica foi utilizada para analisar se a relação entre as variáveis `prob` (a probabiliade p1 acha que p2 tem de querer se encontrar novamente), `int_corr` (a correlação entre os interesses de p1 e p2), `like` (no geral, quanto p1 gostou de p2), `intel` (quão inteligente p1 achou p2), `attr` (quão atraente p1 achou p2) possuem uma associação com a a probabilidade de haver match entre dois participabntes do encontro. 

Os resultados da regressão indicam que um modelo com os 5 preditores no formato $log(\frac{p(y)}{1-p(y)}) = -0,66 + 0,78*prob + 0.006*int_corr + 0.005*like + 0.006*intel + 0.005*attr$ explicam 51,55% da variância da variável de resposta (R2 = 0.5155). 

#Como aqui y = exp(b0)*exp(b1*x1), aumentar em uma unidade x, faz com que y seja multiplicado por exp(b1), que é o estimate nessa tabela acima.

#Como aqui, y = 0,0014*(1,19*prob), aumentar em uma unidade `prob`, faz com que a probabilidade de dar match seja multiplicada por 1,19.

* A variável $prob$, medida como o nível de conceito da instituição tem uma relação positiva e relevante com o erro b = [0.50; 1.06], IC com 95%

* A variável $int_corr$, medida como a quantidade total de publicações em qualis A1 com a participação de alunos, tem uma relação positiva e pouco relevante (ou talvez nem possuir, pelo fato do intervalo de confiança incluir o 0) com o erro b = [-0.01; 0.02], IC com 95%

* A variável $intel$, medida como a porcetagem de docentes colaboradores, pode possuir uma  relação negativa e pouco relevante (ou talvez nem possuir, pelo fato do intervalo de confiança incluir o 0) com o erro b = [-0.07; 0.05], IC com 95%. 

* A variável $int_corr$, medida como a quantidade total de publicações em qualis A1 com a participação de alunos, tem uma relação positiva e pouco relevante (ou talvez nem possuir, pelo fato do intervalo de confiança incluir o 0) com o erro b = [-0.01; 0.02], IC com 95%

* A variável $attr$, medida como a porcetagem de docentes colaboradores, pode possuir uma  relação negativa e pouco relevante (ou talvez nem possuir, pelo fato do intervalo de confiança incluir o 0) com o erro b = [-0.07; 0.05], IC com 95%.

Após análise dos resultados obtidos para os valores das estimativas associadas a cada variável independente, implicam que o aumento de 1 unidade de $prob$ produz uma mudança de 0,78 na produtividade dos programas de pós graduação em Ciência da Computação do Brasil, enquanto que o aumento da variável $periodicosComAlunoA1$ aumenta em 0,006 na produtividade do programa e o aumento do número de $docentesColaboradores$ possui em relação negativa de -0,005 nesta. O modelo só explica aproximadamente 51% da variância do modelo pelo MC Fadden, e consequentemente, é necessaŕio analisar o impacto de outras variáveis para medir a produtividade dos programas.

Portanto, na prática, é possível observar que os de acordo com os dados disponibilizados pela CAPES, mostram que os programas de pós graduação em Ciência da Computação do Brasil mais conceitualizados aparenta ser um fator de bastante influência na produtividade dos programas, enquanto periódicos com qualis A1 com a participação de alunos também aparentam possuir impacto positivo (caso exista), porém menor que o impacto do nível de conceito do curso de pós graduação. E por fim, a participação de docentes colaboradores na pesquisa provavelmente influencia bem menos (caso influencie) na produtividade os programas.  

# Dúvidas:
## Qual a diferença entre as tabelas com e sem exponentiate?
## A interpretação da frase está correta?
## O modelo deve ser reescrito no formato de expoente?

